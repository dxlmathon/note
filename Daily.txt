计划：
1.SSD完全弄懂
2.c++每天做
（不完成1就不干其它的）


1.python写出所有各种损失函数
1.什么是人脸识别
1.交叉熵
1、vector与list的区别、线程与进程的区别
1.softmaxlog
2.假如用一张没有训练过的人脸放入你的识别系统，怎么着？
4.adaboost是线性还是非线性的
5.回归于分类的区别
6.几个搜索算法，包括梯度下降，随机搜索等，还有SGD

总结
1.python中的奇异值分解计算出来的v是v.T，其中有个参数是是不是全部分解，要是false的话，出来的u阵不是方阵。
1.np.unique(); np.setdiff1d(); [i for i in range(10) if i > 5]

1.所谓的权重衰减就是给误差函数加称惩罚项的系数，惩罚项一般是权重的二范数，为了防止某一权重过大造成的过拟合。（对某一特征特别敏感）

1.所谓的逻辑斯蒂回归模型是指：当线性函数的值越接近正无穷，概率值就越接近1；当线性函数的值越接近负无穷，概率值就越接近0。
2.所谓的最大似然估计，就是先假设模型参数，然后计算实验结果的概率是多少，概率越大所对应的参数越接近实际参数。
3.最大熵于原理时概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型时最好的模型。
4.学习的目的时从所有可能的模型中选择最优模型。
1.有时直接导入a然后使用a.b会出现a中没有b这个属性，这时需要直接import a.b
3.winsound.Beep(600, 1000)

1.lambda的用法：匿名函数
ff = lambda x:x**2
print(ff(3))

2.所谓的装饰器，就是调用某函数时先把其放入对应的装饰器中装饰一下。
def f1(func):
    print('fuck')
    return func
@f1
def f2():
    print('you')
f2()

1.python中字符串连接可使用.join
 c = '_'.join(['a', 'b', 'c']), 则c为a_b_c
2.b = a 并不是浅复制，需使用b = a[:]

1.random_state
 为整数时：不随机
 为None时： 随机

2.偏差和方差
泛化误差可以分解为偏差、方差和噪声之和。
偏差度量了学习算法的期望预测和真是结果的偏离程度。
方差度量了同样大小的训练样本集的变动所导致的学习性能的变化，即数据扰动造成的影响。
噪声可以认为是数据自身的波动性，表达了任何学习算法所能达到泛化误差的下限。
偏差大说明欠拟合，方差大说明过拟合。

1. 一层可以使用多个不同尺寸的卷积核，参考GoogleNet，inception
2. 所谓的bottleNeck就是使用1*1的卷积核将计算次数降低
3. skip connection 解决网络层数加深梯度弥散问题，使得梯度更容易流动到浅层网络中去。参考resnet、densenet
4. 分别对通道和区域进行操作，即所谓的depthWise，先对每个通道进行卷积，使用1*1的卷积对卷积后的通道进行操作。
5.所谓的depthWise就是首先对每个通道进行卷积操作，所有有通道的卷积核参数一致，然后再使用1*1的卷积核将所有通道进行求和，维度为输出维度的大小。传说中的Xception网络就是基于以上设计而来。可大大减少参数量。
6.分组卷积对通道进行随机分组。